import os
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin
import time
import re

class Principal:
    """
    Clase principal para realizar el scraping de pel√≠culas extranjeras.

    Esta clase encapsula la l√≥gica necesaria para buscar y obtener informaci√≥n 
    sobre pel√≠culas extranjeras desde el sitio web especificado.

    Attributes:
        year (int): El a√±o actual para el cual se est√° buscando pel√≠culas.
        base_url (str): La URL base para acceder a las p√°ginas de pel√≠culas.

    Methods:
        run(): Inicia el proceso de scraping.
        search_files(): Realiza la b√∫squeda recursiva de archivos en las p√°ginas.
        search_recursive(url): Recursivamente busca enlaces en la p√°gina dada.
        get_info(url): Obtiene y guarda la informaci√≥n de una pel√≠cula espec√≠fica.
    """

    def __init__(self):
        """
        Inicializa la clase Principal con el a√±o de b√∫squeda y la URL base.

        Inicializa el a√±o para el cual se realizar√° la b√∫squeda de pel√≠culas y 
        construye la URL base utilizando el a√±o.
        """
        self.year = 2020
        self.base_url = f'https://visuales.uclv.cu/Peliculas/Extranjeras/{self.year}/'

    def run(self):
        """
        Inicia el proceso de scraping.

        Este m√©todo mide el tiempo de ejecuci√≥n y coordina el proceso de scraping
        llamando al m√©todo `search_files` para comenzar la b√∫squeda.
        """
        start = time.time()

        x = self.base_url.split("/")[-2]
        
        print('üèÉ Scrapping...')
        self.search_files()

        print(f"‚è∞ Demor√≥ {(time.time() - start)/60} minutos")

    def search_files(self):
        """
        Inicia la b√∫squeda de archivos en la URL base.

        Este m√©todo imprime un mensaje de inicio y llama al m√©todo `search_recursive`
        para realizar la b√∫squeda recursiva de archivos en la URL base.
        """
        print(f'Comienza el scrappeo en {self.base_url}...')
        self.search_recursive(self.base_url)

    def search_recursive(self, url):
        """
        Realiza una b√∫squeda recursiva de enlaces en una URL dada.

        Este m√©todo maneja los errores de conexi√≥n e intenta acceder a la p√°gina 
        especificada. Si se encuentra un subdirectorio, se sigue buscando 
        recursivamente; si se encuentra un archivo `.nfo`, se llama al m√©todo 
        `get_info` para obtener la informaci√≥n.

        Args:
            url (str): La URL en la que se realizar√° la b√∫squeda.
        """
        intime = True
        while intime:
            try:
                response = requests.get(url, timeout=15)
                intime = False
            except requests.exceptions.Timeout:
                print("‚ùå Fallo de conexi√≥n")
            except requests.exceptions.ConnectionError:
                print("üí£ Fallo de conexi√≥n")

        if response.status_code == 200:
            soup = BeautifulSoup(response.text, "html.parser")

            falses = 5  # Ignora los primeros 5 enlaces, que usualmente no son relevantes

            for link in soup.find_all("a"):
                if falses:
                    falses -= 1
                    continue

                href = link.get("href")

                if href.endswith("/"):
                    subdirectory_url = urljoin(url, href)
                    print(f'üïµÔ∏è  Buscando en {subdirectory_url}...')
                    self.search_recursive(subdirectory_url)
                else:
                    filename = href.split("/")[-1]
                    if re.search(r'(nfo)$', filename):
                        subdirectory_url = urljoin(url, href)
                        print(f'üïµÔ∏è  Buscando en la p√°gina {subdirectory_url}...')
                        self.get_info(subdirectory_url)
                        return
                    else:
                        print(f'üóëÔ∏è  Archivo no interesante: {url + filename}')

        else:
            print("‚ùå URL incorrecta")

    def get_info(self, url):
        """
        Obtiene y guarda la informaci√≥n de una pel√≠cula espec√≠fica.

        Este m√©todo verifica si la informaci√≥n de la pel√≠cula ya existe en un 
        archivo local. Si no existe, realiza una solicitud a la p√°gina de la pel√≠cula,
        guarda el contenido HTML en un archivo `.txt` y lo guarda en la ubicaci√≥n 
        especificada.

        Args:
            url (str): La URL de la p√°gina de la pel√≠cula.
        """
        name = url.split("/")[-2]
        
        # Verifica si el archivo ya existe
        if not os.path.exists(f'../backend/uh-videos-django/info_extranjeras/{self.year}/{name}.txt'):
            # Hacer una solicitud a la p√°gina web
            response = requests.get(url, timeout=15)

            # Obtener el contenido HTML
            html_content = response.text

            # Guardar el HTML en un archivo .txt
            with open(f'../backend/uh-videos-django/info_extranjeras/{self.year}/{name}.txt', 'w', encoding='utf-8') as file:
                file.write(html_content)

            print(f'El archivo ha sido guardado en {name}.txt')
        else:
            print(f'El archivo ya hab√≠a sido guardado en {name}.txt')

def main():
    """
    Funci√≥n principal que ejecuta el proceso de scraping.

    Esta funci√≥n instancia la clase Principal y llama al m√©todo `run` 
    para iniciar el proceso de scraping.
    """
    search_extranjeras = Principal()
    search_extranjeras.run()

if __name__ == "__main__":
    main()
